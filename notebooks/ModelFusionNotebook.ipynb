{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ce56ee-4059-4dff-a3df-4cce6079d193",
   "metadata": {},
   "source": [
    "# Model Ensembling Notebook\n",
    "\n",
    "This notebook contains the text and experiments regarding the benefits of combining / ensembling models for Page Stream Segmentation. It contains both an overview of a section of related work, as well as an analysis of various ensembling strategies from the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ad350-f127-434d-95b5-5a44b46ed1f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Index\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Ensemble on predictions](#average_ensemble)\n",
    "    - 2.1 [Table with combinations](#table_combi)\n",
    "    - 2.2 [Interesting combination](#interesting)\n",
    "    - 2.3 [Problems](#problems)\n",
    "3. [Earlier Combination](#early)\n",
    "4. [Combining Multiple Models](#multiple) \n",
    "5. [Conclusion](#conclusion)\n",
    "6. [Extra Error Analysis](#extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf8c166-fe19-42dd-b521-b713b41c0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "\n",
    "%run metricutils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656bc77-f036-4303-8693-002c97df9373",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading in model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea531f3-1f86-425b-a853-1fca1c150a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def get_bert_prediction_dict(dataframe):\n",
    "    output = {}\n",
    "    for doc_id, stream in dataframe.groupby('name'):\n",
    "        predictions = np.vstack([np.array(stream['raw_scores_0']), np.array(stream['raw_scores_1'])])\n",
    "        output[doc_id] = F.softmax(torch.from_numpy(predictions), dim=0)[1, :].tolist()\n",
    "        output[doc_id][0] = 1\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faedf347-5902-4d40-9a1a-fc75483fe906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for investigation, also get the binary labels instead of the probabilities\n",
    "def make_bin(probability_dict):\n",
    "    out = {}\n",
    "    for key, value in probability_dict.items():\n",
    "        out[key] = np.array([round(item) for item in value])\n",
    "        out[key][0] = 1\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9b4ac7-8ac0-4eae-95fa-8ddb271f1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_dataframe = pd.concat([pd.read_csv('../resources/datasets/LONG/dataframes/train.csv'), pd.read_csv('../resources/datasets/LONG/dataframes/test.csv')])\n",
    "SHORT_dataframe = pd.concat([pd.read_csv('../resources/datasets/SHORT/dataframes/train.csv'), pd.read_csv('../resources/datasets/SHORT/dataframes/test.csv')])\n",
    "\n",
    "TEXTCNN_predictions_raw_LONG = read_json('../resources/model_predictions/TEXT-CNN/LONG_LONG/raw_scores.json')\n",
    "CNN_predictions_raw_LONG = read_json('../resources/model_predictions/CNN/LONG_LONG/raw_scores.json')\n",
    "EFFICIENTNET_predictions_raw_LONG = read_json('../resources/model_predictions/EFFICIENTNET/LONG_LONG/raw_scores.json')\n",
    "BERT_predictions_raw_LONG = get_bert_prediction_dict(pd.read_csv('../resources/model_predictions/BERT/LONG_LONG/model_output.csv'))\n",
    "\n",
    "TEXTCNN_predictions_raw_SHORT = read_json('../resources/model_predictions/TEXT-CNN/SHORT_SHORT/raw_scores.json')\n",
    "CNN_predictions_raw_SHORT = read_json('../resources/model_predictions/CNN/SHORT_SHORT/raw_scores.json')\n",
    "EFFICIENTNET_predictions_raw_SHORT = read_json('../resources/model_predictions/EFFICIENTNET/SHORT_SHORT/raw_scores.json')\n",
    "BERT_predictions_raw_SHORT = get_bert_prediction_dict(pd.read_csv('../resources/model_predictions/BERT/SHORT_SHORT/model_output.csv'))\n",
    "\n",
    "LONG_gold_standard_train = read_json('../resources/model_predictions/CNN/LONG_train/gold_standard.json')\n",
    "SHORT_gold_standard_train = read_json('../resources/model_predictions/CNN/SHORT_train/gold_standard.json')\n",
    "\n",
    "LONG_gold_standard_test = read_json('../resources/model_predictions/CNN/LONG_LONG/gold_standard.json')\n",
    "SHORT_gold_standard_test = read_json('../resources/model_predictions/CNN/SHORT_SHORT/gold_standard.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec1aad59-0705-49f2-a313-5a3081fa87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = {**LONG_gold_standard_test, **SHORT_gold_standard_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42a64e1-3938-46aa-aa96-8d75682bf800",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_predictions_raw_LONG_robust = get_bert_prediction_dict(pd.read_csv('../resources/model_predictions/BERT/SHORT_LONG/model_output.csv'))\n",
    "BERT_predictions_raw_SHORT_robust = get_bert_prediction_dict(pd.read_csv('../resources/model_predictions/BERT/LONG_SHORT/model_output.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0885e182-86c0-424b-9de7-efbea0653baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_k_standard = read_json('../resources/model_predictions//BERT-K/standard/predictions.json')\n",
    "bert_k_robust = read_json('../resources/model_predictions//BERT-K/robust/predictions.json')\n",
    "\n",
    "bert_k_standard_final = {}\n",
    "bert_k_robust_final = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa2f7fe-e406-427d-b5fe-e1dde1713524",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in bert_k_standard.items():\n",
    "    prediction = np.zeros_like(np.array(value))\n",
    "    num_docs = np.sum(gold_standard[key])\n",
    "    top_k_indices = np.argpartition(value, -num_docs)[-num_docs:]\n",
    "    prediction[top_k_indices] = 1\n",
    "    prediction[0] = 1\n",
    "    bert_k_standard_final[key] = prediction.astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35d5c023-67ad-45ea-9228-87180341660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in bert_k_robust.items():\n",
    "    prediction = np.zeros_like(np.array(value))\n",
    "    num_docs = np.sum(gold_standard[key])\n",
    "    top_k_indices = np.argpartition(value, -num_docs)[-num_docs:]\n",
    "    prediction[top_k_indices] = 1\n",
    "    prediction[0] = 1\n",
    "    bert_k_robust_final[key] = prediction.astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cda944f6-c815-4345-8c2c-cf07eeb58e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bertk_robust.json', 'w') as j:\n",
    "    json.dump(bert_k_robust_final, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cad29658-d00e-4c30-84dd-07965791eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_results = {'VGG16': evaluation_report(LONG_gold_standard_test, make_bin(CNN_predictions_raw_LONG)),\n",
    "             'EFFICIENTNET': evaluation_report(LONG_gold_standard_test, make_bin(EFFICIENTNET_predictions_raw_LONG)),\n",
    "             'BERT': evaluation_report(LONG_gold_standard_test, make_bin(BERT_predictions_raw_LONG)),\n",
    "             'TEXTCNN': evaluation_report(LONG_gold_standard_test, make_bin(TEXTCNN_predictions_raw_LONG))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66751676-ebcb-436b-acc3-143211a310b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_results = {'VGG16': evaluation_report(SHORT_gold_standard_test, make_bin(CNN_predictions_raw_SHORT)),\n",
    "             'EFFICIENTNET': evaluation_report(SHORT_gold_standard_test, make_bin(EFFICIENTNET_predictions_raw_SHORT)),\n",
    "             'BERT': evaluation_report(SHORT_gold_standard_test, make_bin(BERT_predictions_raw_SHORT)),\n",
    "             'TEXTCNN': evaluation_report(SHORT_gold_standard_test, make_bin(TEXTCNN_predictions_raw_SHORT))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca4821ba-2f2a-435e-b4af-e8447f67afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use *args to make the function work with an arbitrary number of input dicts\n",
    "def combine_predictions(*args):\n",
    "    final_predictions = {}\n",
    "    for key in args[0].keys():\n",
    "        combi_prediction = (np.vstack([model[key] for model in args]).sum(axis=0) / len(args))\n",
    "        final_predictions[key] = np.round(combi_prediction).astype(int).tolist()\n",
    "    return final_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaf185-b4bb-47c5-ad6d-290938fe27a2",
   "metadata": {},
   "source": [
    "This function works nicely for the general case, but we might want to experiment specifically with two models, and how we can adjust \n",
    "the balance betweem them to adjust for problems for example when one of the models is really bad, such as in the robst case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73b43a35-e128-4de4-a02a-f7a6ed56b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_predictions(model_1_probabilities: dict, model_2_probabilities: dict, alpha: float= 0.5):\n",
    "    output_dict = {}\n",
    "    for key in model_1_probabilities.keys():\n",
    "        combined_predictions = np.vstack([alpha*np.array(model_1_probabilities[key]), (1-alpha)*np.array(model_2_probabilities[key])])\n",
    "        output_dict[key] = np.round(combined_predictions.sum(axis=0)).astype(int).tolist()\n",
    "        output_dict[key][0] = 1\n",
    "    return output_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa0b8d79-e402-424d-a0f2-18633e2e636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try the best prediction for the model fusion one in the robust setting\n",
    "EFFICIENTNET_predictions_raw_LONG_robust = read_json('../resources/model_predictions/EFFICIENTNET/SHORT_LONG/raw_scores.json')\n",
    "BERT_predictions_raw_LONG_robust = get_bert_prediction_dict(pd.read_csv('../resources/model_predictions/BERT/SHORT_LONG/model_output.csv'))\n",
    "TEXTCNN_predictions_raw_LONG_robust = read_json('../resources/model_predictions/TEXT-CNN/SHORT_LONG/raw_scores.json')\n",
    "\n",
    "EFFICIENTNET_predictions_raw_SHORT_robust = read_json('../resources/model_predictions/EFFICIENTNET/LONG_SHORT/raw_scores.json')\n",
    "BERT_predictions_raw_SHORT_robust = get_bert_prediction_dict(pd.read_csv('../resources/model_predictions/BERT/LONG_SHORT/model_output.csv'))\n",
    "TEXTCNN_predictions_raw_SHORT_robust = read_json('../resources/model_predictions/TEXT-CNN/LONG_SHORT/raw_scores.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b28f22cb-ca9f-4d14-a0a4-c167e388e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-cf5465b5f2f5>:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(LONG_gold_standard_test, combo_robustness_LONG).mean().T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Precision               0.80\n",
       "Recall                  0.58\n",
       "F1                      0.60\n",
       "SQ                      0.77\n",
       "SQ*                     0.77\n",
       "Weighted PQ P           0.55\n",
       "Weighted PQ* P          0.55\n",
       "Weighted PQ R           0.44\n",
       "Weighted PQ* R          0.44\n",
       "Weighted PQ F1          0.47\n",
       "Weighted PQ* F1         0.47\n",
       "Unweighted PQ P         0.59\n",
       "Unweighted PQ* P        0.59\n",
       "Unweighted PQ R         0.48\n",
       "Unweighted PQ* R        0.48\n",
       "Unweighted PQ F1        0.50\n",
       "Unweighted PQ* F1       0.50\n",
       "support              6347.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_robustness_LONG = weigh_predictions(EFFICIENTNET_predictions_raw_LONG_robust, BERT_predictions_raw_LONG_robust, alpha=0.25)\n",
    "evaluation_report(LONG_gold_standard_test, combo_robustness_LONG).mean().T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ac5065e-ae32-4976-860e-da0c07cae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def get_model_correlation(gold_standard, model_1, model_2):\n",
    "    model1_arr = []\n",
    "    model2_arr = []\n",
    "    for key in gold_standard.keys():\n",
    "        model1_correct = (np.array(gold_standard[key]) == np.array(model_1[key])).astype(int)\n",
    "        model2_correct = (np.array(gold_standard[key]) == np.array(model_2[key])).astype(int)\n",
    "        \n",
    "        model1_arr.extend(model1_correct.tolist())\n",
    "        model2_arr.extend(model2_correct.tolist())\n",
    "        \n",
    "    corr = scipy.stats.pearsonr(np.array(model1_arr), np.array(model2_arr))[0]\n",
    "    return corr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39fb3e-17f5-4baa-9dad-b37354b2a15c",
   "metadata": {},
   "source": [
    "<a id=\"intro\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf8fdd-573d-44ae-80f6-b000120c4169",
   "metadata": {},
   "source": [
    "## Model Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72719b42-361b-4b84-a3bf-2bd7e28b8000",
   "metadata": {},
   "source": [
    "As with many fields in Artifical Intelligence, the task of Page Stream Segmentation can benefit from the combination of the predictions of multiple models, where the combination of information form multiple modalities/models can lead to improved performance of the combined system.\n",
    "Methods that combine the information from multiple modalities can be broadly classified into two categories, with either a  combination of the models at the decision level (i.e. the probabilities of the classes), or combination at the feature level.\n",
    "In the case of combining at the feature level, the concatenated feature vectors are then fed into a single model for classification \\cite{attr_mult10}. Which approach is most succesful depends on a number of factors, such as how closely the modalities are to each other \\cite{wu_mult99}.\n",
    "\n",
    "In the case of ensembling at the decision level, \\cite{gune_affe05} have done a study on the most effective way of combining model predictions, in the task of classifying emotions based on facial expressions and body gestures. They concluded that summing the the raw probabilities of both modalities provided the best performance, but that the best method can depend on the specific task and model outputs.\n",
    "\n",
    "In this paper, besides from combining predictions from both the text and image modalities, we also experiment with combining predictions from different classification architecture for the same modality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfa962-a5e8-4dc1-ab9d-f39edec8da08",
   "metadata": {},
   "source": [
    "## Theoretical benefits of ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99445e59-84b0-40f0-883b-c8eefc65a619",
   "metadata": {},
   "source": [
    "As mentioned in \\cite{poli_ense06}, the most benefit from combining different classifiers can be obtained when the classifiers make mistakes on different instances, after which we can select the 'right' classifier with a smart combination strategy. These different classifiers can be obtained in different ways, such as using different initilizations, training on different subsets of the data, or by using different model achitecture. Intuitively this makes sense: If we have the models that make exactly the same mistakes, then there is no 'extra' information we can use, both models always agree.\n",
    "\n",
    "We follow their work, and measure diversity by calculating the number of times both models were correct. We then use the following formula to measure the correlation between te predictions based on contingency table of the model predictions of both models, with a low score indicating diversity. Here, the different numbers indicate the different coordinates in the contingency table.\n",
    "\n",
    "$corr = \\frac{((00*11)-(10*01))}{\\sqrt{(00+10)(01+11)(11+01)(10+11)}}$.\n",
    "\n",
    "This formula looks a bit complicated, but it is equivalent to calculating the Pearson correlation between the vectors of the correctness of model predictions.\n",
    "\n",
    "It is possible to calculate the maximum possible performance of a combined system, given the predictions of the individual models. As done in \\cite{kunc_theo02}, we include an <i>oracle</i> model, which is correct when either of the two models is correct. In this case, False Positives are when both models wrongly predict 1, and False Negatives are when both models incorrectly predict 0.\n",
    "\n",
    "Please note that, for the calculation of the maximum possible performance, we followed the scoring scheme used for the models, meaning that we calculate the maximum possible score per stream, and average this for all the metrics to obtain a final maximum possible score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37598fbd-ab01-4ad9-a1c1-53c1fa218c8e",
   "metadata": {},
   "source": [
    "<a id=\"average_ensemble\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a6b7c-49e5-4c57-8beb-bb0fd646d7db",
   "metadata": {},
   "source": [
    "## Comparing different ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696e4e8-7e90-4581-86db-45d652f881b5",
   "metadata": {},
   "source": [
    "In this section, we will compare various ensemble strategies, both with combining models from different modalities, as well as combining different models from the same modality. We performed a grid search on weigthing schemes and found that average the output probabilities of both models worked best, which we will refer to as <i>average prediction ensemble<i/>.\n",
    "   \n",
    "We have created a table in which we both report the similarity / correlation between model mistakes, as well as to reporting the obtained Panoptic Quality score of best model in the combination, compared to their maximum possible achievable score.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28728a7f-eac0-4bd9-92f1-f54502e25e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def oracle_score(gold_dict, *args):\n",
    "    maximum_score_dict = {}\n",
    "    for key in gold_dict.keys():\n",
    "        combined_prediction = []\n",
    "        gold = np.array(gold_dict[key]).reshape(1, -1)\n",
    "        models = np.stack([model[key] for model in args])\n",
    "        equal = (models == gold).any(axis=0)\n",
    "        maximum_score_dict[key] = np.where(equal, gold, 1-gold).flatten().tolist()\n",
    "        \n",
    "    return maximum_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "004ba839-96ee-4d8d-a1fb-c958e5f08f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LONG_all_predictions = {'TEXTCNN': TEXTCNN_predictions_raw_LONG, 'VGG16': CNN_predictions_raw_LONG,\n",
    "                      'BERT': BERT_predictions_raw_LONG,\n",
    "                     'EFFICIENTNET': EFFICIENTNET_predictions_raw_LONG}\n",
    "\n",
    "SHORT_all_predictions = {'TEXTCNN': TEXTCNN_predictions_raw_SHORT, 'VGG16': CNN_predictions_raw_SHORT,\n",
    "                      'BERT': BERT_predictions_raw_SHORT,\n",
    "                     'EFFICIENTNET': EFFICIENTNET_predictions_raw_SHORT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a34c131-a59a-4a01-9eda-d8c66ea147bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we construct the very large table with all the models and their diversity score + orcale PQ - highest single model PQ\n",
    "def create_confusion_matrix_all_models(gold_standard, all_models_prediction_dict, score: str=\"PQ\",\n",
    "                                      save_path: str = \"\"):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    similarity_matrix = {}\n",
    "    maximum_score_diff = {}\n",
    "\n",
    "    for first_name, first_model in tqdm(all_models_prediction_dict.items()):\n",
    "        similarity_row = {}\n",
    "        score_row = {}\n",
    "        for second_name, second_model in all_models_prediction_dict.items():\n",
    "            # We can use the predictions here to loop over the scores and average them \n",
    "            max_score_predictions = oracle_score(gold_standard, make_bin(first_model), make_bin(second_model))\n",
    "            \n",
    "            if first_name != second_name:\n",
    "                best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
    "                                 evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
    "                max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
    "                score_diff = max_score - best_score\n",
    "                similarity_row[second_name] = get_model_correlation(gold_standard, make_bin(first_model), make_bin(second_model))\n",
    "                score_row[second_name] = score_diff\n",
    "            else:\n",
    "                similarity_row[second_name] = 1\n",
    "                score_row[second_name] = 0\n",
    "        maximum_score_diff[first_name] = score_row\n",
    "        similarity_matrix[first_name] = similarity_row\n",
    "    \n",
    "    def clean_up_matrix(matrix):\n",
    "        matrix.iloc[0, 0] = 0\n",
    "        matrix.iloc[1, 1] = 0\n",
    "        matrix.iloc[2, 2] = 0\n",
    "        matrix.iloc[3, 3] = 0\n",
    "        matrix.iloc[3, [0, 1, 2]] = 0\n",
    "        matrix.iloc[2, [0, 1]] = 0\n",
    "        matrix.iloc[1, 0] = 0\n",
    "        return matrix\n",
    "    \n",
    "    correlation_dataframe = clean_up_matrix(pd.DataFrame(similarity_matrix).round(2))\n",
    "    score_dataframe = clean_up_matrix(pd.DataFrame(maximum_score_diff).round(2))\n",
    "    sns.heatmap(correlation_dataframe, annot=True, ax=axes[0])\n",
    "    axes[0].set_title(\"Pearson correlation between various model combinations\")\n",
    "    axes[1].set_title(\"Difference between the maximum achievable PQ F1 score \\n and the best performing single model of the combination\")\n",
    "    sns.heatmap(score_dataframe, annot=True, ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d651149-eb3e-4d4e-b49f-79ca41ea0c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTCNN + TEXTCNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.81    0.88  0.81  0.91  0.92           0.75            0.75   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.77            0.78            0.75             0.76   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.78              0.78              0.8              0.81   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.78               0.78   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTCNN + VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.87     0.8  0.79  0.93  0.93           0.78            0.78   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.69            0.69            0.71             0.71   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0              0.8               0.8             0.71              0.71   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.74               0.74   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTCNN + BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1   SQ  SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.88  0.84  0.9  0.9           0.79            0.79   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.79            0.79            0.78             0.78   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.82              0.82   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.81               0.81   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTCNN + EFFICIENTNET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.88  0.83  0.93  0.93           0.79            0.79   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.79            0.79            0.77             0.77   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.82              0.82   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0               0.8                0.8   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 1/4 [00:09<00:29,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 + TEXTCNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.87     0.8  0.79  0.93  0.93           0.78            0.78   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.69            0.69            0.71             0.71   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0              0.8               0.8             0.71              0.71   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.74               0.74   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 + VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.86     0.7  0.72  0.92  0.92           0.71            0.72   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.57            0.58            0.61             0.62   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.75              0.76              0.6              0.61   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.64               0.65   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 + BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1   SQ  SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.87    0.83  0.82  0.9  0.9            0.8             0.8   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.75            0.75            0.76             0.76   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.83              0.83             0.77              0.77   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.79               0.79   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 + EFFICIENTNET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.84  0.81  0.93  0.92           0.78            0.78   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.75            0.75            0.74             0.74   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.77              0.78   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.77               0.77   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:19<00:19,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + TEXTCNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1   SQ  SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.88  0.84  0.9  0.9           0.79            0.79   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.79            0.79            0.78             0.78   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.82              0.82   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.81               0.81   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1   SQ  SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.87    0.83  0.82  0.9  0.9            0.8             0.8   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.75            0.75            0.76             0.76   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.83              0.83             0.77              0.77   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.79               0.79   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.84    0.88  0.83  0.91  0.91           0.78            0.78   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.78            0.78            0.77             0.77   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.81              0.81              0.8               0.8   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.79               0.79   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + EFFICIENTNET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.87  0.83  0.94  0.94            0.8             0.8   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.78            0.78            0.78             0.78   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.81              0.81   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0               0.8                0.8   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [00:29<00:09,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFICIENTNET + TEXTCNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.88  0.83  0.93  0.93           0.79            0.79   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.79            0.79            0.77             0.77   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.82              0.82   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0               0.8                0.8   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFICIENTNET + VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.84  0.81  0.93  0.92           0.78            0.78   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.75            0.75            0.74             0.74   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.77              0.78   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.77               0.77   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFICIENTNET + BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall    F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.85    0.87  0.83  0.94  0.94            0.8             0.8   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.78            0.78            0.78             0.78   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.82              0.82             0.81              0.81   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0               0.8                0.8   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFICIENTNET + EFFICIENTNET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a360cfc977a6>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SQ*</th>\n",
       "      <th>Weighted PQ P</th>\n",
       "      <th>Weighted PQ* P</th>\n",
       "      <th>Weighted PQ R</th>\n",
       "      <th>Weighted PQ* R</th>\n",
       "      <th>Weighted PQ F1</th>\n",
       "      <th>Weighted PQ* F1</th>\n",
       "      <th>Unweighted PQ P</th>\n",
       "      <th>Unweighted PQ* P</th>\n",
       "      <th>Unweighted PQ R</th>\n",
       "      <th>Unweighted PQ* R</th>\n",
       "      <th>Unweighted PQ F1</th>\n",
       "      <th>Unweighted PQ* F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall   F1    SQ   SQ*  Weighted PQ P  Weighted PQ* P  \\\n",
       "0       0.82    0.86  0.8  0.92  0.92           0.76            0.76   \n",
       "\n",
       "   Weighted PQ R  Weighted PQ* R  Weighted PQ F1  Weighted PQ* F1  \\\n",
       "0           0.75            0.75            0.73             0.74   \n",
       "\n",
       "   Unweighted PQ P  Unweighted PQ* P  Unweighted PQ R  Unweighted PQ* R  \\\n",
       "0             0.78              0.79             0.78              0.78   \n",
       "\n",
       "   Unweighted PQ F1  Unweighted PQ* F1  support  \n",
       "0              0.76               0.76   6347.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:39<00:00,  9.93s/it]\n"
     ]
    }
   ],
   "source": [
    "for first_name, first_model in tqdm(LONG_all_predictions.items()):\n",
    "    for second_name, second_model in LONG_all_predictions.items():\n",
    "        model_scores = pd.DataFrame(evaluation_report(LONG_gold_standard_test, combine_predictions(first_model, second_model)).mean()).T.round(2)\n",
    "        print(\"%s + %s\" % (first_name, second_name))\n",
    "        display(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd941899-1b4a-4436-bf35-ec760fde6b22",
   "metadata": {},
   "source": [
    "<a id=\"table_combi\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3edbe3b3-a440-4a6a-a352-7df63da07139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      " 25%|██▌       | 1/4 [00:24<01:13, 24.34s/it]<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      " 50%|█████     | 2/4 [00:43<00:42, 21.41s/it]<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      " 75%|███████▌  | 3/4 [01:03<00:20, 20.90s/it]<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "<ipython-input-37-906309eb5fb6>:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
      "<ipython-input-37-906309eb5fb6>:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
      "<ipython-input-37-906309eb5fb6>:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
      "100%|██████████| 4/4 [01:25<00:00, 21.40s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_fusion_images/contingency_LONG.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-41ab8a97fb82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# replace image cnn with vgg16 after I fix this weird error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcreate_confusion_matrix_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLONG_gold_standard_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_all_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model_fusion_images/contingency_LONG.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-906309eb5fb6>\u001b[0m in \u001b[0;36mcreate_confusion_matrix_all_models\u001b[0;34m(gold_standard, all_models_prediction_dict, score, save_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m                         ax.patch._cm_set(facecolor='none', edgecolor='none'))\n\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3046\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2317\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2320\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 **kwargs)\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mDECORATORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \"\"\"\n\u001b[1;32m    540\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2295\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2297\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_fusion_images/contingency_LONG.png'"
     ]
    }
   ],
   "source": [
    "# replace image cnn with vgg16 after I fix this weird error\n",
    "create_confusion_matrix_all_models(LONG_gold_standard_test, LONG_all_predictions, save_path=\"model_fusion_images/contingency_LONG.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250e460-eb03-49b6-93f4-79eb5216c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix_all_models(SHORT_gold_standard_test, SHORT_all_predictions, save_path=\"model_fusion_images/contingency_SHORT.png\")\n",
    "# sometimes this assertion error pops up in the evaluation, not sure why its only between bert and efficientnet, which is extra weird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad920ed-8519-4bfc-9f9c-975df837c904",
   "metadata": {},
   "source": [
    "In the above two heatmaps we can see the diversity score and the possible performance gain of all model model combinations, respectively. What is interesting to note is that in the left table we can see that in general, the models that predict the same modality (EFFICIENTNET, CNN), (BERT, TEXTCNN) have lower diversity scores than when compared to models that concern a different modality. This already confirms that hypothesis that these models from different modalities capture different aspects of the data. Apart from this, we can see that all the models have a very high diversity score when compared the LSTM model. This is mostly due to the fact that the LSTM model has lower accuracy and therefore makes more mistakes, increasing the diversity with the models that are more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we construct the very large table with all the models and their diversity score + orcale PQ - highest single model PQ\n",
    "def create_new_plot_all_models(gold_standard, all_models_prediction_dict, score: str=\"PQ\",\n",
    "                                      save_path: str = \"\"):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    similarity_matrix = {}\n",
    "    maximum_score_diff = {}\n",
    "\n",
    "    for first_name, first_model in tqdm(all_models_prediction_dict.items()):\n",
    "        similarity_row = {}\n",
    "        score_row = {}\n",
    "        for second_name, second_model in all_models_prediction_dict.items():\n",
    "            # We can use the predictions here to loop over the scores and average them \n",
    "            max_score_predictions = oracle_score(gold_standard, make_bin(first_model), make_bin(second_model))\n",
    "            \n",
    "            if first_name != second_name:\n",
    "                best_score = max(evaluation_report(gold_standard, make_bin(first_model)).mean().T['Weighted PQ F1'],\n",
    "                                 evaluation_report(gold_standard, make_bin(second_model)).mean().T['Weighted PQ F1'])\n",
    "                max_score = evaluation_report(gold_standard, max_score_predictions).mean().T['Weighted PQ F1']\n",
    "                score_diff = max_score - best_score\n",
    "                similarity_row[second_name] = get_model_correlation(gold_standard, make_bin(first_model), make_bin(second_model))\n",
    "                score_row[second_name] = score_diff\n",
    "            else:\n",
    "                similarity_row[second_name] = 1\n",
    "                score_row[second_name] = 0\n",
    "        maximum_score_diff[first_name] = score_row\n",
    "        similarity_matrix[first_name] = similarity_row\n",
    "    \n",
    "    def clean_up_matrix(matrix):\n",
    "        matrix.iloc[0, 0] = 0\n",
    "        matrix.iloc[1, 1] = 0\n",
    "        matrix.iloc[2, 2] = 0\n",
    "        matrix.iloc[3, 3] = 0\n",
    "        matrix.iloc[3, [0, 1, 2]] = 0\n",
    "        matrix.iloc[2, [0, 1]] = 0\n",
    "        matrix.iloc[1, 0] = 0\n",
    "        return matrix\n",
    "    \n",
    "    def get_dict_combinations(d):\n",
    "        output_dict = {'TEXT-CNN & VGG16': d['TEXTCNN']['VGG16'],\n",
    "        'TEXT-CNN & BERT': d['TEXTCNN']['BERT'],\n",
    "        'TEXT-CNN & EFFICIENTNET': d['TEXTCNN']['EFFICIENTNET'],\n",
    "        'VGG16 & BERT': d['VGG16']['BERT'],\n",
    "        'VGG16 & EFFICIENTNET': d['VGG16']['EFFICIENTNET'],\n",
    "        'EFFICIENTNET & BERT': d['EFFICIENTNET']['BERT']}\n",
    "        return output_dict\n",
    "    \n",
    "    combination_barplot = pd.DataFrame({'correlation': get_dict_combinations(similarity_matrix),\n",
    "                          'oracle': get_dict_combinations(maximum_score_diff)})\n",
    "    return combination_barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace image cnn with vgg16 after I fix this weird error\n",
    "df = create_new_plot_all_models(SHORT_gold_standard_test, SHORT_all_predictions, save_path=\"../model_fusion_images/contingency_SHORT.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\n",
    "df = df.sort_values(by='correlation', ascending=False)\n",
    "df.index = ['(I + T) ', '(T + I) ', '(I + T) ', '(T + I) ', '(I + I) ', '(T + T) '] + df.index\n",
    "df.plot(kind='barh', ax=ax)\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "ax.plot(df['correlation'], range(len(df)), color='black', marker='o')\n",
    "ax.plot(df['oracle'], range(len(df)), color='black', marker='o')\n",
    "plt.savefig('images/modelf_fusion_plot.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe92b2-6b1f-40c1-a598-8367c2d0e029",
   "metadata": {},
   "source": [
    "<a id=\"interesting\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58257612-466f-4454-9326-32f152d9a34e",
   "metadata": {},
   "source": [
    "## Interesting combination of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a909a2e-50ed-49a2-bbad-cc05830397b8",
   "metadata": {},
   "source": [
    "From the above observations of both datasets, we found that although the LSTM combinations showed the most improvement, this was mostly due to the fact that the predictions were less accurate. Because there is also quite a large possible improvement possible in the BERT-EFFICIENTNET combination, we took this as our example pair, and looked into it in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446d0be-c42f-4e8f-b990-93ccc569fcc9",
   "metadata": {},
   "source": [
    "**BERT+EFF LONG**\n",
    "| Model         | Page P    | Page R | Page F1 | Doc. PQ | Doc. SQ |Doc.  RQ F1|\n",
    "|         :--:  |     ---:  |   ---: | ---:| ---:| ---: | ---: |\n",
    "| EFFICIENTNET  |  0.82     |  0.85  | 0.80 | 0.76 | 0.95 | 0.76 |\n",
    "| BERT          |  0.84     |  0.88  | 0.83 | 0.78 | 0.97 | 0.79 | \n",
    "| AVG. ENSEMBLE |  0.85     |  0.87  | 0.83 | 0.80 | 0.96 | 0.80 |\n",
    "| ORACLE        |  0.92     |  0.94  | 0.91 | 0.89 | 0.97 | 0.89 |\n",
    "\n",
    "\n",
    "**BERT+EFF SHORT**\n",
    "| Model         | Page P    | Page R | Page F1 | Doc. PQ | Doc. SQ |Doc.  RQ F1 |\n",
    "|        :--:   |       ---: |    ---: | ---:| ---:|---: |---: |\n",
    "| EFFICIENTNET  |  0.83     |  0.75  |0.75|0.74|0.92|0.71|\n",
    "| BERT          |  0.80     |  0.76  |0.73|0.67|0.92|0.66|\n",
    "| AVG. ENSEMBLE |  0.85     |  0.75  |0.75|0.71|0.92|0.69|\n",
    "| ORACLE        |  0.91     |  0.85  |0.85|0.83|0.96|0.80|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e80ba3-9d5a-4d28-8c16-da3aa6d05916",
   "metadata": {},
   "source": [
    "From the above results, we can see that the performance of the combined model is better than the individual model, but we are not able to actually reach the maximum potential of the combination. The most likely explanation for this is because of the fact that during training, the models are incentivised to push their predictions close to 0 or 1, due to the cross entropy loss. This partly explains why it is hard to optimize, as both models are generally really confident in their predictions, even when they are wrong. Thus if both models are really confident and they disagree, it is hard to know which one is correct, and as in late fusion you only have the probabilities to go by, the ability to learn this is also very limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d0300-2070-4469-817a-490853c92fe6",
   "metadata": {},
   "source": [
    "We will try to quantatively show the problem with the raw binary predictions, we will do this by showing their average scores, both when the models are correct, and when the models are incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7d9f7-8feb-491b-beae-af626e2412c6",
   "metadata": {},
   "source": [
    "<a id=\"problems\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf828c81-862f-4a73-a1dd-b1310e61453d",
   "metadata": {},
   "source": [
    "### Problems with binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895917c-cbff-41da-a6a9-4f5c9e038c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_raw_model_scores(gold_standard, model_predictions, model: str):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    binary_predictions = make_bin(model_predictions)\n",
    "    \n",
    "    gold_standard_label = []\n",
    "    binary_label = []\n",
    "    model_score = []\n",
    "    \n",
    "    for key in gold_standard.keys():        \n",
    "        gold_standard_label.extend(gold_standard[key])\n",
    "        binary_label.extend(binary_predictions[key])\n",
    "        model_score.extend(model_predictions[key])\n",
    "        \n",
    "    scores_df = pd.DataFrame({'gold': gold_standard_label, 'prediction': binary_label, 'prob': model_score})\n",
    "    sns.kdeplot(data=scores_df, x='prob', clip=[0, 1], ax=axes[0])\n",
    "    axes[0].set_title(\"Distribution of the raw probability scores\\n for the %s model\" % model)\n",
    "    # Apart from analysing the model only by plotting the scores in general, we also plot the scores\n",
    "    # for the model when it is actually incorrect, we do this by taking a subsample of the data\n",
    "    incorrect_predictions = scores_df[scores_df.gold != scores_df.prediction]\n",
    "    sns.kdeplot(data=incorrect_predictions, x='prob', clip=[0,1], ax=axes[1], hue='prediction')\n",
    "    axes[1].set_title(\"Distribution of the probability scores for the %s model\\n when the prediction was incorrect\" % model)\n",
    "    plt.savefig(\"images/model_fusion_images/binary_bert.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cdd24-8336-4078-baff-137c60827c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_raw_model_scores(LONG_gold_standard_test, BERT_predictions_raw_LONG, model=\"BERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e7e32-9012-4dfd-8cfb-5a8456eeb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_prediction(gold_standard, model_predictions, model: str):\n",
    "    binary_predictions = make_bin(model_predictions)\n",
    "    \n",
    "    gold_standard_label = []\n",
    "    binary_label = []\n",
    "    model_score = []\n",
    "    \n",
    "    for key in gold_standard.keys():        \n",
    "        gold_standard_label.extend(gold_standard[key])\n",
    "        binary_label.extend(binary_predictions[key])\n",
    "        model_score.extend(model_predictions[key])\n",
    "        \n",
    "    scores_df = pd.DataFrame({'gold': gold_standard_label, 'prediction': binary_label, 'prob': model_score})\n",
    "    sns.kdeplot(data=scores_df, x='prob', clip=[0, 1])\n",
    "    #plt.title(\"Distribution of the raw probability scores\\n for the %s model\" % model)\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.savefig('images/bert_probabilities.eps', format='eps')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4263f-0ae1-433b-aedb-c9125ffcef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_prediction(LONG_gold_standard_test, BERT_predictions_raw_LONG, model=\"BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b27ef-532f-47f7-b44e-3c9382f5c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_raw_model_scores(LONG_gold_standard_test, CNN_predictions_raw_LONG, model=\"TEXTCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5b945-c647-4b2a-b5d2-7e2bada19c31",
   "metadata": {},
   "source": [
    "Although it is not the case in the same extreme for all models, there are definitely a few models, such as the efficientnet model shown above, for which the predictions are very 'binary', even when the model is wrong, as we can see in the plot on the right. We also see the same kind of behaviour for the BERT, VGG16 model and the LSTM model, but less so for the TEXTCNN model, where the predictions are a bit closer to 0.5 when the model is incorrect, while still being very confident in correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69d898-fcd2-4f2d-a9c7-2dccf066920f",
   "metadata": {},
   "source": [
    "### Plot from the two different classifiers when they disagree, seperated for 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d8bff-ca8e-4127-97f2-349bd20bb792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyse_model_differences(model1_predictions, model2_predictions, model1: str, model2: str):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    model1_binary = make_bin(model1_predictions)\n",
    "    model2_binary = make_bin(model2_predictions)\n",
    "    \n",
    "    model1_label = []\n",
    "    model2_label = []\n",
    "    \n",
    "    model1_score = []\n",
    "    model2_score = []\n",
    "    \n",
    "    for key in model1_binary.keys():        \n",
    "        model1_label.extend(model1_binary[key])\n",
    "        model2_label.extend(model2_binary[key])\n",
    "        \n",
    "        model1_score.extend(model1_predictions[key])\n",
    "        model2_score.extend(model2_predictions[key])\n",
    "        \n",
    "    \n",
    "    scores_df = pd.DataFrame({'model1_pred': model1_label, 'model2_pred': model2_label, 'prob_model1': model1_score, 'prob_model2': model2_score})\n",
    "    # plot two different ones, the first one where model1 predicts 0, and the other one where it predicts 1\n",
    "    model_differences = scores_df[scores_df.model1_pred != scores_df.model2_pred]\n",
    "\n",
    "    sns.kdeplot(data=model_differences, x='prob_model2', hue='model1_pred', clip=[0, 1], ax=axes[0])\n",
    "    sns.kdeplot(data=model_differences, x='prob_model1', hue='model2_pred', clip=[0, 1], ax=axes[1])\n",
    "    axes[0].set_xlabel(model2)\n",
    "    axes[1].set_xlabel(model1)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c6984-eb9d-4131-ba48-4e80c4522b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mistake_types(gold_standard, model1_predictions, model2_predictions):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    model1_binary = make_bin(model1_predictions)\n",
    "    model2_binary = make_bin(model2_predictions)\n",
    "    \n",
    "    model1_label = []\n",
    "    model2_label = []\n",
    "    gold_standard_label = []\n",
    "    \n",
    "    model1_score = []\n",
    "    model2_score = []\n",
    "    \n",
    "    for key in model1_binary.keys():        \n",
    "        model1_label.extend(model1_binary[key])\n",
    "        model2_label.extend(model2_binary[key])\n",
    "        gold_standard_label.extend(gold_standard[key])\n",
    "        \n",
    "        model1_score.extend(model1_predictions[key])\n",
    "        model2_score.extend(model2_predictions[key])\n",
    "        \n",
    "    \n",
    "    scores_df = pd.DataFrame({'gold': gold_standard_label, 'model1_pred': model1_label, 'model2_pred': model2_label, 'prob_model1': model1_score, 'prob_model2': model2_score})\n",
    "    # plot two different ones, the first one where model1 predicts 0, and the other one where it predicts 1\n",
    "    model_differences = scores_df[scores_df.model1_pred != scores_df.model2_pred]\n",
    "    print(model_differences['gold'].value_counts(normalize=True))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfaea4a-86d6-40a4-9e56-32d61b4edf18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyse_model_differences(EFFICIENTNET_predictions_raw_LONG, BERT_predictions_raw_LONG, model1=\"EFFICIENTNET\", model2=\"BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb2c30-ad05-47d7-890f-7af23c8a55cc",
   "metadata": {},
   "source": [
    "The above plot shows what happens when the BERT and EFFICIENTNET models disagree. In the case of the BERT model, if the EFFICENTNET model predicted something different the score are very binary. For the EFFICIENTNET bart, the scores are also quite binary, but a bit less so than for efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93931d-68aa-4ce3-b317-5d3e65d04cea",
   "metadata": {},
   "source": [
    "We also briefly looked into for which class the models disagreed the most, and it turns at that this is quite balanced with 54\\% of the disagreement being for the 0 class, and 46\\% for the 1 class. Given that the distribution of classes is quite skewed, this means the 1 class is a bit overrepresented in the disagreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c44ad7-659f-4e0c-a557-fb49db6c0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_types(LONG_gold_standard_test, EFFICIENTNET_predictions_raw_LONG, BERT_predictions_raw_LONG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b592cba-c8b7-496d-ad79-74ba66dd6a60",
   "metadata": {},
   "source": [
    "### Investigating the PQ score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c6cd1-ea0c-4df8-941c-9be069e7e3b8",
   "metadata": {},
   "source": [
    "Apart from the interesting observations about the binary preditions, we can also see that the model combination has a large possible improvement in the PQ scores, while the SQ score is almost perfect, but the RQ score is not. To investigate this further, we will split up this RQ score into Precision Recall and F1 to see where the biggest improvement is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392fcab-6e4b-494a-b323-4026cd969ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score of our combined model\n",
    "EFF_BERT_ensemble = combine_predictions(EFFICIENTNET_predictions_raw_LONG, BERT_predictions_raw_LONG)\n",
    "oracle_predictions = oracle_score(LONG_gold_standard_test, make_bin(BERT_predictions_raw_LONG), make_bin(EFFICIENTNET_predictions_raw_LONG))\n",
    "\n",
    "EFF_BERT_ensemble_scores = evaluation_report(LONG_gold_standard_test, EFF_BERT_ensemble)\n",
    "oracle_scores = evaluation_report(LONG_gold_standard_test, oracle_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096f0e7-96d4-471a-b151-f0331a46802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(EFF_BERT_ensemble_scores.loc['RQ']).T.iloc[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1856fd-9ddf-4cd0-8579-97829403f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(oracle_scores.loc['RQ']).T.iloc[:, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c3595-10d0-4eab-9d7e-5b34299eba54",
   "metadata": {},
   "source": [
    "This tells us that the performance potential missed by the models is not attributed to a drop in precision or recall only, but rather both although recall has a bit of a larger gap than precision. We will dig a bit deeper to find the issue here. The segmentation quality measures how well True Positives are lined up, summing IoU values over all TP values. This means that the model is able to match true positives very well. For the regocnition quality, we can investigate this by simply retrieving the false positives and false negatives that the model produces, and investigate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2637d4-5a72-433b-9262-c367bc4403f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'align' function for this\n",
    "combined_model_mistakes = {}\n",
    "oracle_model_mistakes = {}\n",
    "for key in EFF_BERT_ensemble.keys():\n",
    "    _, _,FP, FN = align(bin_to_length_list(LONG_gold_standard_test[key]), bin_to_length_list(EFF_BERT_ensemble[key]), kind=\"and\")\n",
    "    combined_model_mistakes[key] = {'FP': FP, 'FN': FN}\n",
    "    \n",
    "oracle_mistakes = {}\n",
    "for key in oracle_predictions.keys():\n",
    "    _, _,FP, FN = align(bin_to_length_list(LONG_gold_standard_test[key]), bin_to_length_list(oracle_predictions[key]), kind=\"and\")\n",
    "    oracle_model_mistakes[key] = {'FP': FP, 'FN': FN}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44569aa1-6523-4d1e-a273-8c831a711c3b",
   "metadata": {},
   "source": [
    "Now that we have the mistakes for both the combination and the oracle, we can try to compare these on a stream basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99722188-eeec-4ca1-9a2e-b97fe8a6fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Even kijken hoe ik hier goed fouten kan vergelijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b416b2-66aa-4f4e-b226-334841e91c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in combined_model_mistakes.keys():\n",
    "    combined_FP_mistakes = combined_model_mistakes[key]['FP'] - oracle_model_mistakes[key]['FP']\n",
    "    combined_FN_mistakes = combined_model_mistakes[key]['FN'] - oracle_model_mistakes[key]['FN']\n",
    "    #print(combined_FP_mistakes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba2b94-669c-4fcf-9028-2ba7d0456fa8",
   "metadata": {},
   "source": [
    "<a id=\"early\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb4818-5e8b-4fa1-a85e-272b69f764fa",
   "metadata": {},
   "source": [
    "# Combining earlier layers of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc18d4e-8040-486e-9be7-899e039b113f",
   "metadata": {},
   "source": [
    "As we can see from the results above, by the time we combine the predictions on the decision level, we are actually already 'too late', i.e. the models have become really confident in their predictions, and combining them is problematic because of this. This poses a good reason for trying to combine the information from the models at an earlier stage, to make use of more rich information, and to avoid the problems of the binary predictions that we saw above. This technique is referred to as joint / hybrid fusion or ensebmling. To do this, we are now going to use the last linear layer for all the models, and use a simple logistic regression on the concatenated features, and then report the scores of the models, as well as to rerun the analysis on the output probabilities of the Logistic Regression to see whether the problem has been alleviated. We also normalize the concatenated embeddings to ensure they are of the same magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543e999-734f-47fa-b566-b6619fc53009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load in the logistic regression.\n",
    "# Note that we have to train this, we will train this on the train sets of the models and then we will test it on the test vectors of the models.\n",
    "\n",
    "vector_base_path = '../resources/model_predictions'\n",
    "\n",
    "TEXTCNN_train_vectors_LONG = np.load(os.path.join(vector_base_path, 'TEXT-CNN', 'LONG_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "TEXTCNN_test_vectors_LONG = np.load(os.path.join(vector_base_path, 'TEXT-CNN', 'LONG_LONG', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "TEXTCNN_train_vectors_SHORT = np.load(os.path.join(vector_base_path, 'TEXT-CNN', 'SHORT_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "TEXTCNN_test_vectors_SHORT = np.load(os.path.join(vector_base_path, 'TEXT-CNN', 'SHORT_SHORT', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "\n",
    "VGG16_train_vectors_LONG = np.load(os.path.join(vector_base_path, 'CNN', 'LONG_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "VGG16_test_vectors_LONG = np.load(os.path.join(vector_base_path, 'CNN', 'LONG_LONG', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "VGG16_train_vectors_SHORT = np.load(os.path.join(vector_base_path, 'CNN', 'SHORT_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "VGG16_test_vectors_SHORT = np.load(os.path.join(vector_base_path, 'CNN', 'SHORT_SHORT', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "\n",
    "EFFICIENTNET_train_vectors_LONG = np.load(os.path.join(vector_base_path, 'EFFICIENTNET', 'LONG_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "EFFICIENTNET_test_vectors_LONG = np.load(os.path.join(vector_base_path, 'EFFICIENTNET', 'LONG_LONG', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "EFFICIENTNET_train_vectors_SHORT = np.load(os.path.join(vector_base_path, 'EFFICIENTNET', 'SHORT_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "EFFICIENTNET_test_vectors_SHORT = np.load(os.path.join(vector_base_path, 'EFFICIENTNET', 'SHORT_SHORT', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "\n",
    "BERT_train_vectors_LONG = np.load(os.path.join(vector_base_path, 'BERT', 'LONG_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "BERT_test_vectors_LONG = np.load(os.path.join(vector_base_path,'BERT', 'LONG_LONG', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "BERT_train_vectors_SHORT = np.load(os.path.join(vector_base_path, 'BERT', 'SHORT_train', 'raw_vecs.npy'), allow_pickle=True)[()]\n",
    "BERT_test_vectors_SHORT = np.load(os.path.join(vector_base_path, 'BERT', 'SHORT_SHORT', 'raw_vecs.npy'), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ad9c0-0cec-4701-82b7-6b57acbcb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d86432-7ddc-44df-a58c-62bb766af4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def norm(vectors):\n",
    "    return preprocessing.normalize(vectors, norm='l2')\n",
    "\n",
    "def train_combined_vector_regression(train_gold_standard, test_gold_standard, model1_train_vectors, model1_test_vectors, model2_train_vectors, model2_test_vectors):\n",
    "    logistic_regressor = LogisticRegression(max_iter=500)\n",
    "    # First we create the training data\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    \n",
    "    for key in train_gold_standard.keys():\n",
    "        train_y.extend(train_gold_standard[key])\n",
    "        train_X.append(np.hstack([model1_train_vectors[key], model2_train_vectors[key]]))\n",
    "    \n",
    "    train_X = norm(np.vstack(train_X))\n",
    "    logistic_regressor.fit(train_X, train_y)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Now that we have trained the model, we will test it.\n",
    "    for key in test_gold_standard.keys():\n",
    "        test_X = np.hstack([model1_test_vectors[key], model2_test_vectors[key]])\n",
    "        predictions[key] = logistic_regressor.predict_proba(norm(test_X))[:, 1]\n",
    "        predictions[key][0] = 1\n",
    "        \n",
    "    return predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302cd69-54cd-43ac-85d6-8256e54376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vectors_LONG = {'BERT': [BERT_train_vectors_LONG, BERT_test_vectors_LONG],\n",
    "                   'VGG16': [VGG16_train_vectors_LONG, VGG16_test_vectors_LONG],\n",
    "                   'TEXTCNN': [TEXTCNN_train_vectors_LONG, TEXTCNN_test_vectors_LONG],\n",
    "                   'EFFICIENTNET': [EFFICIENTNET_train_vectors_LONG, EFFICIENTNET_test_vectors_LONG]}\n",
    "model_vectors_SHORT = {'BERT': [BERT_train_vectors_SHORT, BERT_test_vectors_SHORT],\n",
    "                   'VGG16': [VGG16_train_vectors_SHORT, VGG16_test_vectors_SHORT],\n",
    "                   'TEXTCNN': [TEXTCNN_train_vectors_SHORT, TEXTCNN_test_vectors_SHORT],\n",
    "                   'EFFICIENTNET': [EFFICIENTNET_train_vectors_SHORT, EFFICIENTNET_test_vectors_SHORT]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3055a6-4f42-4a2f-9c17-5b6684641baf",
   "metadata": {},
   "source": [
    "<a id=\"multiple\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3753a09-feff-4c3b-aa40-98e9f46a9cf3",
   "metadata": {},
   "source": [
    "Now that we have written the code to use the classifier, we will train the models of all combinations and show its results when compared to predictions level combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c996e96-8bb2-4585-86ec-e0cb6dd17acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "early_combos_LONG = {}\n",
    "combinations = ['BERT-VGG16', 'BERT-TEXTCNN', 'BERT-EFFICIENTNET', 'VGG16-TEXTCNN', \"VGG16-EFFICIENTNET\",\n",
    "               'TEXTCNN-EFFICIENTNET']\n",
    "for model1_name in tqdm(model_vectors_LONG.keys()):\n",
    "    for model2_name in model_vectors_LONG.keys():\n",
    "        if \"%s-%s\" % (model1_name, model2_name) in combinations:\n",
    "            print(\"Combination of %s and %s\" % (model1_name, model2_name))\n",
    "            logistic_reg_predictions = train_combined_vector_regression(LONG_gold_standard_train, LONG_gold_standard_test,\n",
    "                                                                        *model_vectors_LONG[model1_name], *model_vectors_LONG[model2_name])\n",
    "            model_combination = combine_predictions(LONG_all_predictions[model1_name], LONG_all_predictions[model2_name])\n",
    "            early_scores = evaluation_report(LONG_gold_standard_test, make_bin(logistic_reg_predictions)).loc[[\"Boundary\", \"PQ\"], :]\n",
    "            late_scores = evaluation_report(LONG_gold_standard_test, make_bin(model_combination)).loc[[\"Boundary\", \"PQ\"], :]\n",
    "            early_combos_LONG[\"%s-%s\" % (model1_name, model2_name)] = {'early': early_scores.loc['PQ', 'recall'], 'late': late_scores.loc['PQ', 'recall']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a680196-2931-4952-9091-ad18e66f0a1e",
   "metadata": {},
   "source": [
    "Surprisingly, we can see from the above results that the outputs of training the logistic regression are not on the same level as the model combination on the prediction level. However, it does seem to work well for the CNN and TEXTCNN combination, where performance is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8ed42-56b7-4892-a721-46959b6e660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "early_combos_SHORT = {}\n",
    "combinations = ['BERT-VGG16', 'BERT-TEXTCNN', 'BERT-EFFICIENTNET', 'VGG16-TEXTCNN', \"VGG16-EFFICIENTNET\",\n",
    "               'TEXTCNN-EFFICIENTNET']\n",
    "for model1_name in tqdm(model_vectors_SHORT.keys()):\n",
    "    for model2_name in model_vectors_SHORT.keys():\n",
    "        if \"%s-%s\" % (model1_name, model2_name) in combinations:\n",
    "            print(\"Combination of %s and %s\" % (model1_name, model2_name))\n",
    "            logistic_reg_predictions = train_combined_vector_regression(SHORT_gold_standard_train, SHORT_gold_standard_test,\n",
    "                                                                        *model_vectors_SHORT[model1_name], *model_vectors_SHORT[model2_name])\n",
    "            model_combination = combine_predictions(SHORT_all_predictions[model1_name], SHORT_all_predictions[model2_name])\n",
    "            early_scores = evaluation_report(SHORT_gold_standard_test, make_bin(logistic_reg_predictions)).loc[[\"Boundary\", \"PQ\"], :]\n",
    "            late_scores = evaluation_report(SHORT_gold_standard_test, make_bin(model_combination)).loc[[\"Boundary\", \"PQ\"], :]\n",
    "            early_combos_SHORT[\"%s-%s\" % (model1_name, model2_name)] = {'early': early_scores.loc['PQ', 'recall'], 'late': late_scores.loc['PQ', 'recall']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f0d36-8cba-4e4b-a800-b56b2cdc9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "pd.DataFrame(early_combos_LONG).T.plot(kind='bar', ax=axes[0])\n",
    "pd.DataFrame(early_combos_SHORT).T.plot(kind='bar', ax=axes[1])\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True)\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True)\n",
    "plt.savefig('images/early_ensemble_recall.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e4a7d-9174-4acd-a870-969540b5748c",
   "metadata": {},
   "source": [
    "## Combining information from more than two classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad9177-be3b-4345-8c96-298687711475",
   "metadata": {},
   "source": [
    "As a final experiment, we will try to combine the information from four classifiers, where we will again take an average decision level average, and see what the outcome is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc4a49-3348-4a08-aa10-c485a72e55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_models_LONG = evaluation_report(LONG_gold_standard_test, combine_predictions(BERT_predictions_raw_LONG, TEXTCNN_predictions_raw_LONG, CNN_predictions_raw_LONG, EFFICIENTNET_predictions_raw_LONG))\n",
    "oracle_scores_multi_LONG = evaluation_report(LONG_gold_standard_test, oracle_score(LONG_gold_standard_test, combine_predictions(BERT_predictions_raw_LONG, TEXTCNN_predictions_raw_LONG, CNN_predictions_raw_LONG, EFFICIENTNET_predictions_raw_LONG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b69ce-54e9-4fbe-b336-24b949f3f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_models_SHORT = evaluation_report(SHORT_gold_standard_test, combine_predictions(BERT_predictions_raw_SHORT, TEXTCNN_predictions_raw_SHORT, CNN_predictions_raw_SHORT, EFFICIENTNET_predictions_raw_SHORT))\n",
    "oracle_scores_multi_SHORT = evaluation_report(SHORT_gold_standard_test, oracle_score(SHORT_gold_standard_test, combine_predictions(BERT_predictions_raw_SHORT, TEXTCNN_predictions_raw_SHORT, CNN_predictions_raw_SHORT, EFFICIENTNET_predictions_raw_SHORT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101bc12-fe6a-4d53-bf63-5ed895d18dc1",
   "metadata": {},
   "source": [
    "To make this a bit more insightful, I will make a group barplot in which we can show some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f6739-6e5b-49de-ae6c-ec4abaf4b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_barplot_scores(model_dataframe):\n",
    "    return [*model_dataframe.loc['Boundary', :].iloc[:3].values.tolist(), model_dataframe.loc['PQ']['F1'], model_dataframe.loc['SQ']['F1'], model_dataframe.loc['RQ']['F1']]\n",
    "   \n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\n",
    "metrics = [\"Page P\", \"Page R\", \"Page F1\", \"Doc F1\", \"SQ\", \"RQ\"]\n",
    "\n",
    "single_models_LONG = [pd.DataFrame({model: collect_barplot_scores(scores)}) for model, scores in LONG_results.items()]\n",
    "textcnn_bert_scores_LONG = evaluation_report(LONG_gold_standard_test, combine_predictions(TEXTCNN_predictions_raw_LONG, BERT_predictions_raw_LONG))\n",
    "efficientnet_bert_scores_LONG = evaluation_report(LONG_gold_standard_test, combine_predictions(EFFICIENTNET_predictions_raw_LONG, BERT_predictions_raw_LONG))\n",
    "combined_models_LONG = [pd.DataFrame({'BERT+TEXTCNN': collect_barplot_scores(textcnn_bert_scores_LONG)}),\n",
    "                                pd.DataFrame({'BERT+EFFICIENTNET':collect_barplot_scores(efficientnet_bert_scores_LONG)}), pd.DataFrame({'ALL_MODELS': collect_barplot_scores(multiple_models_LONG)})]\n",
    "\n",
    "single_models_SHORT = [pd.DataFrame({model: collect_barplot_scores(scores)}) for model, scores in SHORT_results.items()]\n",
    "textcnn_bert_scores_SHORT = evaluation_report(SHORT_gold_standard_test, combine_predictions(TEXTCNN_predictions_raw_SHORT, BERT_predictions_raw_SHORT))\n",
    "efficientnet_bert_scores_SHORT = evaluation_report(SHORT_gold_standard_test, combine_predictions(EFFICIENTNET_predictions_raw_SHORT, BERT_predictions_raw_SHORT))\n",
    "combined_models_SHORT = [pd.DataFrame({'BERT+TEXTCNN': collect_barplot_scores(textcnn_bert_scores_SHORT)}),\n",
    "                                pd.DataFrame({'BERT+EFFICIENTNET':collect_barplot_scores(efficientnet_bert_scores_SHORT)}), pd.DataFrame({'ALL_MODELS': collect_barplot_scores(multiple_models_SHORT)})]\n",
    "\n",
    "pd.concat([*single_models_LONG, *combined_models_LONG],\n",
    "    axis=1).plot.bar(ax=axes[0])\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].set_title(\"Scores of the single model models, two combinations of models and the combination\\n of all models on LONG\")\n",
    "\n",
    "pd.concat([*single_models_SHORT, *combined_models_SHORT],\n",
    "    axis=1).plot.bar(ax=axes[1])\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].set_title(\"Scores of the single model models, two combinations of models and the combination\\n of all models on SHORT\")\n",
    "plt.tight_layout()\n",
    "axes[0].legend(loc=\"lower center\")\n",
    "axes[1].legend(loc=\"lower center\")\n",
    "plt.savefig(\"images/all_models_barplots.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d9f15-d5af-4056-9b84-a1a61ad2dce9",
   "metadata": {},
   "source": [
    "Unsurprisingly, the model that combines all models  as the best, slightly outperforming the best 2model combination. We also tried incorporating the LSTM model, but this decreased the overall performane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39c905-be8f-427e-b4b4-26d9809f9de3",
   "metadata": {},
   "source": [
    "<a id=\"extra\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d22769-d00c-44cd-8829-a27695e22619",
   "metadata": {},
   "source": [
    "## Extra: Hard Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fe70e-32d7-47c2-910d-213f1d251d84",
   "metadata": {},
   "source": [
    "As another small extra we looked at the following: What pages did all the models predict wrong? This might help us identify some particularly hard phenomena or maybe some labelling mistakes. What is interesting to see is that there are quite a few examples where the models all wrongly predicted samples of consequtive pages, we will show some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7765a9-a7f6-4aa4-91e2-21df85357b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models_wrong(gold_standard, *args):\n",
    "    mistakes = {}\n",
    "    \n",
    "    for key in gold_standard.keys():\n",
    "        model_predictions = np.vstack([np.array(model[key]).round() for model in args]).T\n",
    "        # make a column for each model otherwise numpy gives errors\n",
    "        tiled_gold = np.tile(gold_standard[key], reps=len(args)).reshape(-1, len(args))\n",
    "        model_correctness = (tiled_gold == model_predictions).astype(int)\n",
    "        \n",
    "        # now find rows with only zeros, where all models were incorrect\n",
    "        all_wrong = np.where(~model_correctness.any(axis=1))[0]\n",
    "        mistakes[key] = {'pages': (all_wrong+1).tolist(), 'correct': np.array(gold_standard[key])[all_wrong]} \n",
    "    return mistakes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd86d1e-07d8-4cdb-a944-c0f234bf8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_all_wrong = all_models_wrong(LONG_gold_standard_test, BERT_predictions_raw_LONG, TEXTCNN_predictions_raw_LONG, CNN_predictions_raw_LONG, EFFICIENTNET_predictions_raw_LONG)\n",
    "SHORT_all_wrong = all_models_wrong(SHORT_gold_standard_test, BERT_predictions_raw_SHORT, TEXTCNN_predictions_raw_SHORT, CNN_predictions_raw_SHORT, EFFICIENTNET_predictions_raw_SHORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dc1dd-cf0d-4ba8-80a4-c8fb44d53d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(SHORT_all_wrong.keys())[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0976d8-8cd0-43cb-9ce3-9aef892e5902",
   "metadata": {},
   "source": [
    "I have manually selected some images which we show here to see why all models get these wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56598325-5121-4899-8bb8-342817ede394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image_dict = '/Users/rvanheusden/Downloads/png/'\n",
    "def plot_model_mistakes(mistakes_dict, specific_stream: str = None, specific_range = None):\n",
    "    if specific_stream:\n",
    "        info = mistakes_dict[specific_stream]\n",
    "        pages = info['pages']\n",
    "        ground_truth = info['correct']\n",
    "        image_paths = [os.path.join(image_dict, specific_stream+'-%d.png' % num) for num in pages]\n",
    "        for i, path in enumerate(image_paths):\n",
    "            img = mpimg.imread(path)\n",
    "            imgplot = plt.imshow(img)\n",
    "            plt.xlabel(\"True label % d, models predicted %d\" % (ground_truth[i], 1-ground_truth[i]))\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8ae42-582b-4e50-b04d-c5a5ddb476ff",
   "metadata": {},
   "source": [
    "Although it is hard to see exactly, it seems that some of these issues with the consecutive pages come from emails. This is acutally not surprising as it seems emails are part of the same document sometimes and then concatenated, or they are part of a different document, but in either case there is one email per page, and the models really don't have anything to go by."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec89b20-b2af-459d-8b74-ffa25a7e2e25",
   "metadata": {},
   "source": [
    "We kunnen dit hard maken door een kleine email classifier te maken op de tekst en dan te zien waar de fouten vandaan komen (zowel voor LONG als SHORT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07635182-d16c-4dc4-a97e-54e11000d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_mistakes(dataframe, dict_of_mistakes):\n",
    "    page_text = []\n",
    "    for key, value in dict_of_mistakes.items():\n",
    "        for page in value['pages']:\n",
    "            page_text.append(dataframe[(dataframe['name'] == key) & (dataframe['page'] == page)]['text'].tolist()[0])\n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02756e-d977-48fe-b466-e3d1abdcbe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LONG_mistakes_text = get_text_from_mistakes(LONG_dataframe, LONG_all_wrong)\n",
    "#SHORT_mistakes_text = get_text_from_mistakes(SHORT_dataframe, SHORT_all_wrong)\n",
    "\n",
    "numbers_wrong_LONG = len([item for key in LONG_all_wrong.keys() for item in LONG_all_wrong[key]['pages']])\n",
    "numbers_wrong_SHORT = len([item for key in SHORT_all_wrong.keys() for item in SHORT_all_wrong[key]['pages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66444283-8e96-442a-85ce-ac1262f3825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numbers_wrong_LONG)\n",
    "print(numbers_wrong_SHORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a6084-0a90-4b24-9a33-74cf996b02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_classifier(page):\n",
    "    if \"van\" in page.lower() and \"aan\" in page.lower() and \"onderwerp\" in page.lower() and \"verzonden\" in page.lower() and \"cc\" in page.lower():\n",
    "        return True\n",
    "    elif \"from\" in page.lower() and \"to\" in page.lower() and \"subject\" in page.lower() and \"sent\" in page.lower() and \"cc\" in page.lower():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "106acf09-f726-4c3a-b56a-a4f77fcffe50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D2_all_wrong' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ec4cb44a2908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD2_all_wrong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD2_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD2_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'page'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0memail_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'D2_all_wrong' is not defined"
     ]
    }
   ],
   "source": [
    "for key, value in SHORT_all_wrong.items():\n",
    "    for i, page in enumerate(value['pages']):\n",
    "        text = SHORT_dataframe[(SHORT_dataframe['name'] == key) & (SHORT_dataframe['page'] == page)]['text'].tolist()[0]\n",
    "        if email_classifier(text):\n",
    "            pass\n",
    "            # print(key, page, value['correct'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fed541-18f5-4b89-b2db-44536dadc5ac",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5ef38-6e0d-4564-afe0-1716987e2a65",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we investigated the usefulness of ensembling different PSS classifiers. We found that although the usage of combining the models through decision level ensembling yielded some performance gain, the usefulness was limited due to the models outputting very binary predictions. To this extent, we investigated the usage of hybrid ensembling to circumvent this problem, but found that it had a negative effect on the overall performance of most models. Finally we tried combining all models to form a prediction with decision level ensembling, and found that this yielded a performance gain, at the case of having to train all models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DutchRoberta",
   "language": "python",
   "name": "dutchroberta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
