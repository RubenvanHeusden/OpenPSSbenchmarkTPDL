{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8779192b-efb1-46b3-b467-6ead95cf064f",
   "metadata": {},
   "source": [
    "# Simple Supervised baseline models\n",
    "\n",
    "In this notebook we will implement several simple baselines from the scikit-learn package, that are not fixed are capable of learning, and that serve as a good baseline for the more complex neural-based approaches that we use in the paper.\n",
    "\n",
    "We will run K-nearest Neighbours and XGBoost. We will do both the classic and robust experiments, and we will finish both sections with a table containing all the approaches we used and the scores they achieved, where we report Page P, R and F1, and Document SQ, RQ and PQ.\n",
    "\n",
    "We will also try the robustness experiments on the XGBoost and the KNN models and see if combining both modalities leads to better scores than just using the individual modalities.\n",
    "\n",
    "## Index\n",
    "1. [Dataloading and Setting Up](#dataloading)\n",
    "2. [XGBoost](#xgboost)\n",
    "3. [K-Nearest Neighbours](#knearestneighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bc6a3-7970-444a-83e2-7c0218c7c974",
   "metadata": {},
   "source": [
    "<a id=\"dataloading\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42afeec-eb5c-42b5-b2f8-337e9b59bce5",
   "metadata": {},
   "source": [
    "## Loading in the data and setting up\n",
    "\n",
    "As the first step we will load in the data that we will use for the experiments. This will consist of the pretrained image-and text vectors for both datasets, and the gold standard data on the document boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40799df4-fdc3-4aeb-ac95-6543d2362847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e58370b-8c13-4e33-a76f-1f257545c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_json(dataframe):\n",
    "    output_dict = {}\n",
    "    for doc_id, doc_data in dataframe.groupby('name'):\n",
    "        output_dict[doc_id] = doc_data['label'].tolist()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efcc7ea5-9134-4fe7-b1c4-1d22c9be66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gold standard\n",
    "# Load train and test for LONG\n",
    "LONG_train = pandas_to_json(pd.read_csv('../resources/datasets/LONG/dataframes/train.csv'))\n",
    "LONG_test = pandas_to_json(pd.read_csv('../resources/datasets/LONG/dataframes/test.csv'))\n",
    "                         \n",
    "# Load train and test for SHORT\n",
    "SHORT_train = pandas_to_json(pd.read_csv('../resources/datasets/SHORT/dataframes/train.csv'))\n",
    "SHORT_test = pandas_to_json(pd.read_csv('../resources/datasets/SHORT/dataframes/test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae3e50-8eb9-4f15-a576-daf643d24291",
   "metadata": {},
   "source": [
    "Next up is loading all the pre-trained vectors for both datasets and both modalities, which are all saved in json dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "804dd0a7-5b55-4442-9807-9995ab6eeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image vectors for both datasets\n",
    "LONG_image_train_vectors = np.load('../resources/page_vectors/image_vectors/LONG/train_vectors.npy', allow_pickle=True)[()]\n",
    "LONG_image_test_vectors = np.load('../resources/page_vectors/image_vectors/LONG/test_vectors.npy', allow_pickle=True)[()]\n",
    "\n",
    "SHORT_image_train_vectors = np.load('../resources/page_vectors/image_vectors/SHORT/train_vectors.npy', allow_pickle=True)[()]\n",
    "SHORT_image_test_vectors = np.load('../resources/page_vectors/image_vectors/SHORT/test_vectors.npy', allow_pickle=True)[()]\n",
    "\n",
    "\n",
    "# Load the text vectors for both datasets\n",
    "LONG_text_train_vectors = np.load('../resources/page_vectors/bert_vectors/LONG/train_vectors.npy', allow_pickle=True)[()]\n",
    "LONG_text_test_vectors = np.load('../resources/page_vectors/bert_vectors/LONG/test_vectors.npy', allow_pickle=True)[()]\n",
    "\n",
    "SHORT_text_train_vectors = np.load('../resources/page_vectors/bert_vectors/SHORT/train_vectors.npy', allow_pickle=True)[()]\n",
    "SHORT_text_test_vectors = np.load('../resources/page_vectors/bert_vectors/SHORT/test_vectors.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7704a3da-c3d4-4c55-8272-72051ac9dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combination vectors for the multimodal experiments\n",
    "LONG_combined_train_vectors = {key: np.concatenate([LONG_text_train_vectors[key], LONG_image_train_vectors[key]], axis=1) for key in LONG_train.keys()}\n",
    "LONG_combined_test_vectors = {key: np.concatenate([LONG_text_test_vectors[key], LONG_image_test_vectors[key]], axis=1) for key in LONG_test.keys()}\n",
    "\n",
    "SHORT_combined_train_vectors = {key: np.concatenate([SHORT_text_train_vectors[key], SHORT_image_train_vectors[key]], axis=1) for key in SHORT_train.keys()}\n",
    "SHORT_combined_test_vectors = {key: np.concatenate([SHORT_text_test_vectors[key], SHORT_image_test_vectors[key]], axis=1) for key in SHORT_test.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3413f3-bd83-4959-b252-248582bc1c53",
   "metadata": {},
   "source": [
    "Now that we have all the data we need we can almost start doing the experiments, we just have to convert this format with json into numpy arrays so that we can immediately use them with scikit-learn. we will write a quick helper function to do this. We won't have to do this for our test data, as we will use a custom function for prediction that works with the format our evaluation metrics expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab52867e-2709-4b44-849a-3935b8b9fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_sklearn(x_json: dict, y_json: dict):\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    for key in x_json.keys():\n",
    "        X_data.append(x_json[key])\n",
    "        y_data.extend(y_json[key])\n",
    "        \n",
    "    return np.concatenate(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7ffc1bd-c014-45f2-a2a8-4c520e6bd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the training data\n",
    "\n",
    "# For the images\n",
    "LONG_image_X_train, LONG_image_y_train = json_to_sklearn(LONG_image_train_vectors, LONG_train)\n",
    "\n",
    "SHORT_image_X_train, SHORT_image_y_train = json_to_sklearn(SHORT_image_train_vectors, SHORT_train)\n",
    "\n",
    "# For the text\n",
    "LONG_text_X_train, LONG_text_y_train = json_to_sklearn(LONG_text_train_vectors, LONG_train)\n",
    "\n",
    "SHORT_text_X_train, SHORT_text_y_train = json_to_sklearn(SHORT_text_train_vectors, SHORT_train)\n",
    "\n",
    "# For the combined/ multimodal vectors\n",
    "LONG_combo_X_train, LONG_combo_y_train = json_to_sklearn(LONG_combined_train_vectors, LONG_train)\n",
    "\n",
    "SHORT_combo_X_train, SHORT_combo_y_train = json_to_sklearn(SHORT_combined_train_vectors, SHORT_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744ec83-0d83-402d-81e3-ebaed690a8be",
   "metadata": {},
   "source": [
    "Wit the dataloading done, the only thing left to do is to import the evaluation functions. We will also define our own predict wrapper for the functions, as we want the predictions in a dictionary on a stream level to integrate nicely with our metrics code, which scikit-learn does not provide natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3720aeec-074e-4bbe-8edf-7c9f0280b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metricutils file\n",
    "%run metricutils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ab6ec-ccdc-40ba-904e-0f3468b9d313",
   "metadata": {},
   "source": [
    "## Simple Baselines\n",
    "\n",
    "Here we calculate the two degenerate baselines first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e1ded18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(sklearn_model, test_data_dict: dict, gold_standard, known_k:bool = True):\n",
    "    output_predictions = {}\n",
    "    for stream_id, vectors in tqdm(test_data_dict.items()):\n",
    "        if known_k:\n",
    "            num_breaks = sum(gold_standard[stream_id])\n",
    "            preds = sklearn_model.predict_proba(vectors)[:, 1].squeeze()\n",
    "            most_confident_breaks = np.argpartition(preds, -num_breaks)[-num_breaks:]\n",
    "            preds[most_confident_breaks] = 1\n",
    "            preds[preds < 1] = 0\n",
    "        else:\n",
    "            preds = sklearn_model.predict(vectors).squeeze()\n",
    "        output_predictions[stream_id] = preds.tolist()\n",
    "        output_predictions[stream_id][0] = 1\n",
    "    return output_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b2fb5ef-da41-4637-983c-4fad56cad116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that does an experiment and returns the results in the format that we want\n",
    "def experiment(sklearn_model, train_x: np.ndarray, train_y: np.ndarray, test_x_dict: dict, test_y_dict: dict,\n",
    "              known_k: bool=False):\n",
    "    # train the model\n",
    "    sklearn_model.fit(train_x, train_y)\n",
    "    \n",
    "    # make the predictions\n",
    "    predictions = model_predict(sklearn_model, test_x_dict, test_y_dict, known_k=known_k)\n",
    "    \n",
    "    # evaluate\n",
    "    return predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efe03b-da82-4e2a-ad21-59596f95ae83",
   "metadata": {},
   "source": [
    "<a id=\"xgboost\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020731d-9ec6-401e-a9da-f466b3716c73",
   "metadata": {},
   "source": [
    "## Experiment 1: XGBoost\n",
    "\n",
    "Next up is trying As XGBoost to see if we can improve over the performance of the logistic regression model. As XGBoost is not natively available in scikit-learn, we have to install it as a separate package. Luckily it does have very good integration with scikit-learn making it possible to pretty much use it like any other classifier. As with logistic regression we tried some different hyperparameters, and show the best performing model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a4ebe5a-5b7a-4d97-a75c-4cdc9c4ba6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf1f19b1-5e09-48ae-91a8-32b5b641ee36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-af54160af49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m LONG_text_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_text_X_train, LONG_text_y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                              LONG_text_test_vectors, LONG_test, known_k=False)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m SHORT_text_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_text_X_train, SHORT_text_y_train,\n\u001b[1;32m      5\u001b[0m                              SHORT_text_test_vectors, SHORT_test, known_k=False)\n",
      "\u001b[0;32m<ipython-input-33-cd461d12faf0>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(sklearn_model, train_x, train_y, test_x_dict, test_y_dict, known_k)\u001b[0m\n\u001b[1;32m      3\u001b[0m               known_k: bool=False):\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msklearn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# make the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1488\u001b[0m             )\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LONG_text_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_text_X_train, LONG_text_y_train,\n",
    "                             LONG_text_test_vectors, LONG_test, known_k=False)\n",
    "\n",
    "SHORT_text_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_text_X_train, SHORT_text_y_train,\n",
    "                             SHORT_text_test_vectors, SHORT_test, known_k=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f50909a-2376-4e86-82e3-0ced8c38453a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5ccb41826346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m LONG_image_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_image_X_train, LONG_image_y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                              LONG_image_test_vectors, LONG_test, known_k=False)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m SHORT_image_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_image_X_train, SHORT_image_y_train,\n\u001b[1;32m      5\u001b[0m                              SHORT_image_test_vectors, SHORT_test, known_k=False)\n",
      "\u001b[0;32m<ipython-input-33-cd461d12faf0>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(sklearn_model, train_x, train_y, test_x_dict, test_y_dict, known_k)\u001b[0m\n\u001b[1;32m      3\u001b[0m               known_k: bool=False):\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msklearn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# make the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m             )\n\u001b[0;32m-> 1471\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1472\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    447\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 448\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_evaluation_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalsLog\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n\u001b[1;32m    961\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DutchRoberta/lib/python3.9/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_numpy_array\u001b[0;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     _check_call(\n\u001b[0;32m--> 214\u001b[0;31m         _LIB.XGDMatrixCreateFromDense(\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0m_array_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LONG_image_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_image_X_train, LONG_image_y_train,\n",
    "                             LONG_image_test_vectors, LONG_test, known_k=False)\n",
    "\n",
    "SHORT_image_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_image_X_train, SHORT_image_y_train,\n",
    "                             SHORT_image_test_vectors, SHORT_test, known_k=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6dbca-0049-46be-b87b-56e85345f3ef",
   "metadata": {},
   "source": [
    "Apart from these two unimodal models we also run a model based on both modalities, where we simply concatenate the features of both input modalities.\n",
    "Here we do use normalization to get the features into the right range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af3227-a088-46ef-9ee4-3bebdf82db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "LONG_scaler = StandardScaler()\n",
    "LONG_train_vecs = LONG_scaler.fit_transform(LONG_combo_X_train)\n",
    "\n",
    "LONG_combined_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_train_vecs, LONG_combo_y_train,\n",
    "                             {key: LONG_scaler.transform(value) for key , value in LONG_combined_test_vectors.items()}, LONG_test)\n",
    "\n",
    "SHORT_scaler = StandardScaler()\n",
    "SHORT_train_vecs = SHORT_scaler.fit_transform(SHORT_combo_X_train)\n",
    "\n",
    "SHORT_combined_results_xgboost = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_train_vecs, SHORT_combo_y_train,\n",
    "                             {key: SHORT_scaler.transform(value) for key , value in SHORT_combined_test_vectors.items()}, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4eeb2-f1f1-4ef9-bd36-c26ae37dfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_results_LONG = pd.DataFrame({'XGBOOST-TEXT': LONG_text_results_xgboost[1], 'XGBOOST-IMAGE': LONG_image_results_xgboost[1],\n",
    "                                  'XGBOOST-MULTI': LONG_combined_results_xgboost[1]}).T\n",
    "xgboost_results_SHORT = pd.DataFrame({'XGBOOST-TEXT': SHORT_text_results_xgboost[1], 'XGBOOST-IMAGE': SHORT_image_results_xgboost[1],\n",
    "                                  'XGBOOST-MULTI': SHORT_combined_results_xgboost[1]}).T\n",
    "xgboost_results = pd.concat([xgboost_results_LONG, xgboost_results_SHORT], axis=1, keys=['LONG', 'SHORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fcb25-6b5a-41fd-9063-5510a968094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5f87a-07c9-4c76-9f7f-0118cf7bee75",
   "metadata": {},
   "source": [
    "<a id=\"knearestneighbours\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef6cad-2117-4172-8479-725089f8e37a",
   "metadata": {},
   "source": [
    "## Experiment 2: K-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153ef9e-43fb-46dd-958a-6dcf256c94b7",
   "metadata": {},
   "source": [
    "As a final experiment in this classic setup, we will also using K-Nearest neighbours to do the classification, following the same experimental setup as with the previous two approaches. Naturally, we do some testing on the optimal value of K by using a grid search over the possible values of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dad57c91-8a13-4097-9325-62407895da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55709340-a678-4cef-8a68-bebd9b1e5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:53<00:00,  1.57s/it]\n",
      "100%|██████████| 108/108 [00:23<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "LONG_text_results_knn = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), LONG_text_X_train, LONG_text_y_train,\n",
    "                             LONG_text_test_vectors, LONG_test, known_k=True)\n",
    "\n",
    "SHORT_text_results_knn = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), SHORT_text_X_train, SHORT_text_y_train,\n",
    "                             SHORT_text_test_vectors, SHORT_test, known_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcbe67d3-e4c5-4800-bd2b-e57f7bb6b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:31<00:00,  4.47s/it]\n",
      "100%|██████████| 108/108 [01:35<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# and now do the image\n",
    "LONG_image_results_knn = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), LONG_image_X_train, LONG_image_y_train,\n",
    "                             LONG_image_test_vectors, LONG_test, known_k=True)\n",
    "\n",
    "SHORT_image_results_knn = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), SHORT_image_X_train, SHORT_image_y_train,\n",
    "                             SHORT_image_test_vectors, SHORT_test, known_k=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb3806-a856-4536-8bb6-f3c0a44a0bcf",
   "metadata": {},
   "source": [
    "Now we do the same thing for the image classifier, finding optimal values of K for both datasets and run the multimodel version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c38add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the results so that we can put them in the large dataframe later\n",
    "json_dump({**LONG_image_results_knn, **SHORT_image_results_knn}, '../../experiment_notebooks/experiment_results/KNN-IMAGE-K/predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6418e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the results so that we can put them in the large dataframe later\n",
    "json_dump({**LONG_text_results_knn, **SHORT_text_results_knn}, '../../experiment_notebooks/experiment_results/KNN-TEXT-K/predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d87a7f-2723-43df-bcb9-0f7186cf3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_scaler = StandardScaler()\n",
    "LONG_train_vecs = LONG_scaler.fit_transform(LONG_combo_X_train)\n",
    "\n",
    "LONG_combined_results_knn = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), LONG_train_vecs, LONG_combo_y_train,\n",
    "                             {key: LONG_scaler.transform(value) for key , value in LONG_combined_test_vectors.items()}, LONG_test)\n",
    "\n",
    "SHORT_scaler = StandardScaler()\n",
    "SHORT_train_vecs = SHORT_scaler.fit_transform(SHORT_combo_X_train)\n",
    "\n",
    "SHORT_combined_results_knn = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), SHORT_train_vecs, SHORT_combo_y_train,\n",
    "                             {key: SHORT_scaler.transform(value) for key , value in SHORT_combined_test_vectors.items()}, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e425a3f-4fd6-4742-be9c-c11f50e0efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results_LONG = pd.DataFrame({'KNN-TEXT': LONG_text_results_knn[1], 'KNN-IMAGE': LONG_image_results_knn[1], 'KNN-MULTI': LONG_combined_results_knn[1]}).T\n",
    "knn_results_SHORT = pd.DataFrame({'KNN-TEXT': SHORT_text_results_knn[1], 'KNN-IMAGE': SHORT_image_results_knn[1], 'KNN-MULTI': SHORT_combined_results_knn[1]}).T\n",
    "knn_results = pd.concat([knn_results_LONG, knn_results_SHORT], axis=1, keys=['LONG', 'SHORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9201a-edaa-4897-9f76-0b39274bf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11e476e7-c742-49d4-b37c-4c26fd731948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_dump(dictionary: dict, filepath: str):\n",
    "    with open(filepath, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bff5f-2277-4c03-a2ac-c73cb9783bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the results so that we can put them in the large dataframe later\n",
    "json_dump(SHORT_combined_results_knn[0], '../experiment_notebooks/experiment_results/KNN-BOTH/SHORT_SHORT/predictions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8198353-def5-47f2-92c2-36d1fdcc19de",
   "metadata": {},
   "source": [
    "## Final Ranking\n",
    "\n",
    "Now that we  have run our experiments for all three baselines we can simply combine these dataframes and print the final leaderboard, shere we sort and Document Weighted F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6948c-8e71-46f2-8ea0-987692f7fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([xgboost_results, knn_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa92370-cc3f-48cb-a081-fc880788093b",
   "metadata": {},
   "source": [
    "## Robustnesss\n",
    "\n",
    "Apart from running the baselines on the standard task as we have done above, we can also run our models on the robust task with just a few changes to the code, and see what kind of scores we get in that scenario. We will save the results in json dictionaries, so that we can include the models in the plots that we will make of the other models and get a sense of their robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfef03-5c71-4f6d-a2ce-f782c31deed0",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40acca39-c5dd-4091-a56b-c161cccddec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f81c3362250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# first we will try the knn method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m D1_text_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), D2_text_X_train, D2_text_y_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                              D1_text_test_vectors, D1_test)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m D2_text_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), D1_text_X_train, D1_text_y_train,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# first we will try the knn method\n",
    "LONG_text_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), SHORT_text_X_train, SHORT_text_y_train,\n",
    "                             LONG_text_test_vectors, LONG_test)\n",
    "\n",
    "SHORT_text_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), LONG_text_X_train, LONG_text_y_train,\n",
    "                             SHORT_text_test_vectors, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "617ed494-78d6-4fde-9acf-bf99a6dd65f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b96418e0ffc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m D1_image_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), D2_image_X_train, D2_image_y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                              D1_image_test_vectors, D1_test)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m D2_image_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), D1_image_X_train, D1_image_y_train,\n\u001b[1;32m      5\u001b[0m                              D2_image_test_vectors, D2_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "LONG_image_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), SHORT_image_X_train, SHORT_image_y_train,\n",
    "                             LONG_image_test_vectors, LONG_test)\n",
    "\n",
    "SHORT_image_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=5, weights='distance'), LONG_image_X_train, LONG_image_y_train,\n",
    "                             SHORT_image_test_vectors, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bb1589c0-2707-45f7-8376-993257423ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "LONG_scaler = StandardScaler()\n",
    "LONG_train_vecs = LONG_scaler.fit_transform(LONG_combo_X_train)\n",
    "\n",
    "SHORT_scaler = StandardScaler()\n",
    "SHORT_train_vecs = SHORT_scaler.fit_transform(SHORT_combo_X_train)\n",
    "\n",
    "\n",
    "LONG_combined_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), SHORT_train_vecs, SHORT_combo_y_train,\n",
    "                             {key: LONG_scaler.transform(value) for key , value in LONG_combined_test_vectors.items()}, LONG_test)\n",
    "\n",
    "SHORT_combined_results_knn_robust = experiment(KNeighborsClassifier(n_neighbors=25, weights='distance'), LONG_train_vecs, LONG_combo_y_train,\n",
    "                             {key: SHORT_scaler.transform(value) for key , value in SHORT_combined_test_vectors.items()}, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ff4dd2e-39cb-45a5-a5e3-87513714aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results_LONG_robust = pd.DataFrame({'KNN-TEXT': LONG_text_results_knn_robust[1], 'KNN-IMAGE': LONG_image_results_knn_robust[1], 'KNN-MULTI': LONG_combined_results_knn_robust[1]}).T\n",
    "knn_results_SHORT_robust = pd.DataFrame({'KNN-TEXT': SHORT_text_results_knn_robust[1], 'KNN-IMAGE': SHORT_image_results_knn_robust[1], 'KNN-MULTI': SHORT_combined_results_knn_robust[1]}).T\n",
    "knn_results_robust = pd.concat([knn_results_LONG_robust, knn_results_SHORT_robust], axis=1, keys=['LONG', 'SHORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8fe4d2e9-b934-407a-9165-daf0e1e386ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{6}{l}{D1} & \\multicolumn{6}{l}{D2} \\\\\n",
      "{} & Page P & Page R & Page F1 & Doc. SQ & Doc. F1 & Doc W F1 & Page P & Page R & Page F1 & Doc. SQ & Doc. F1 & Doc W F1 \\\\\n",
      "\\midrule\n",
      "KNN-TEXT  &   0.54 &   0.40 &    0.40 &    0.80 &    0.29 &     0.30 &   0.52 &   0.58 &    0.47 &    0.78 &    0.38 &     0.32 \\\\\n",
      "KNN-IMAGE &   0.69 &   0.37 &    0.36 &    0.84 &    0.23 &     0.30 &   0.57 &   0.53 &    0.49 &    0.81 &    0.41 &     0.38 \\\\\n",
      "KNN-MULTI &   0.67 &   0.48 &    0.47 &    0.86 &    0.34 &     0.38 &   0.52 &   0.67 &    0.52 &    0.83 &    0.42 &     0.34 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-232-5793419e35f9>:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(knn_results_robust.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(knn_results_robust.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdba003-297b-4dea-9e54-092324e29a05",
   "metadata": {},
   "source": [
    "## XGBoost Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a8b85deb-ef4b-4406-a4c8-b07ee0918870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will run XGBOOST with robustness\n",
    "LONG_text_results_xgboost_robust = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_text_X_train, SHORT_text_y_train,\n",
    "                             LONG_text_test_vectors, LONG_test)\n",
    "\n",
    "SHORT_text_results_xgboost_robust = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_text_X_train, LONG_text_y_train,\n",
    "                             SHORT_text_test_vectors, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "dbe4f42a-508d-4dc6-a8e4-72890bb5114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_image_results_xgboost_robust = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_image_X_train, SHORT_image_y_train,\n",
    "                             LONG_image_test_vectors, LONG_test)\n",
    "\n",
    "SHORT_image_results_xgboost_robust = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_image_X_train, LONG_image_y_train,\n",
    "                             SHORT_image_test_vectors, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "73b5dc1f-4b5d-4e1c-88fe-c15ecb6db36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "LONG_scaler = StandardScaler()\n",
    "LONG_train_vecs = LONG_scaler.fit_transform(LONG_combo_X_train)\n",
    "\n",
    "SHORT_scaler = StandardScaler()\n",
    "SHORT_train_vecs = SHORT_scaler.fit_transform(SHORT_combo_X_train)\n",
    "\n",
    "\n",
    "LONG_combined_results_xgboost_robust = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), SHORT_train_vecs, SHORT_combo_y_train,\n",
    "                             {key: LONG_scaler.transform(value) for key , value in LONG_combined_test_vectors.items()}, LONG_test)\n",
    "\n",
    "SHORT_combined_results_xgboost_robust = experiment(xgb.XGBClassifier(objective='binary:logistic', booster='gbtree', verbosity=0), LONG_train_vecs, LONG_combo_y_train,\n",
    "                             {key: SHORT_scaler.transform(value) for key , value in SHORT_combined_test_vectors.items()}, SHORT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8c436a9f-377f-419d-89f8-9cbfa3e2fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the results so that we can put them in the large dataframe later\n",
    "json_dump(SHORT_combined_results_xgboost_robust[0], '../experiment_notebooks/experiment_results/XGBOOST-BOTH/LONG_SHORT/predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "64d172da-ad39-4807-95cc-7e0c37dfa3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## boilerplate code to combine predictions of the models on the separate datasets and put them into one folder.\n",
    "import os\n",
    "def combine_predictions(input_root_folder, f1, f2):\n",
    "    first_set = read_json(os.path.join(input_root_folder, f1, 'predictions.json'))\n",
    "    second_set = read_json(os.path.join(input_root_folder, f2, 'predictions.json'))\n",
    "    out = 'standard' if f1 == 'LONG_LONG' else 'robust'\n",
    "    combined_set = {**first_set, **second_set}\n",
    "    json_dump(combined_set, os.path.join(input_root_folder, out, 'predictions.json'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f8e4554f-f244-4dcd-ae7b-027f0439a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_predictions('../experiment_notebooks/experiment_results/XGBOOST-TEXT/', 'SHORT_LONG', 'LONG_SHORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d316c9c7-8680-4c6e-933f-65d0725babd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_results_LONG_robust = pd.DataFrame({'XGBOOST-TEXT': LONG_text_results_xgboost_robust[1], 'XGBOOST-IMAGE': LONG_image_results_xgboost_robust[1], 'XGBOOST-MULTI': LONG_combined_results_xgboost_robust[1]}).T\n",
    "xgboost_results_SHORT_robust = pd.DataFrame({'XGBOOST-TEXT': SHORT_text_results_xgboost_robust[1], 'XGBOOST-IMAGE': SHORT_image_results_xgboost_robust[1], 'XGBOOST-MULTI': SHORT_combined_results_xgboost_robust[1]}).T\n",
    "xgboost_results_robust = pd.concat([xgboost_results_LONG_robust, xgboost_results_SHORT_robust], axis=1, keys=['LONG', 'SHORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1f50569e-8b89-409f-97ee-83d6cbc658f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{6}{l}{D1} & \\multicolumn{6}{l}{D2} \\\\\n",
      "{} & Page P & Page R & Page F1 & Doc. SQ & Doc. F1 & Doc W F1 & Page P & Page R & Page F1 & Doc. SQ & Doc. F1 & Doc W F1 \\\\\n",
      "\\midrule\n",
      "XGBOOST-TEXT  &   0.65 &   0.31 &    0.33 &    0.83 &    0.19 &     0.23 &   0.69 &   0.44 &    0.48 &    0.77 &    0.38 &     0.40 \\\\\n",
      "XGBOOST-IMAGE &   0.67 &   0.35 &    0.36 &    0.83 &    0.23 &     0.30 &   0.44 &   0.60 &    0.45 &    0.81 &    0.34 &     0.27 \\\\\n",
      "XGBOOST-MULTI &   0.66 &   0.36 &    0.33 &    0.85 &    0.22 &     0.27 &   0.67 &   0.50 &    0.50 &    0.79 &    0.41 &     0.41 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-236-1abc84583b39>:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(xgboost_results_robust.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(xgboost_results_robust.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8af1e-e1ae-4dc1-894d-d4bf7a72b29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DutchRoberta",
   "language": "python",
   "name": "dutchroberta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
